{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dd0dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from typing import List, Dict, Union\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import tabula\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "class AdvancedRAGSystem:\n",
    "    def __init__(self):\n",
    "        self.embed_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "        self.chunks = []\n",
    "        self.embeddings = None\n",
    "        self.index = None\n",
    "        self.metadata = []\n",
    "        self.tables_data = []\n",
    "        \n",
    "    def process_pdfs(self, pdf_paths: List[str], chunk_size: int = 1000):\n",
    "        \"\"\"Process multiple PDF files\"\"\"\n",
    "        for pdf_path in pdf_paths:\n",
    "            self.extract_and_process_pdf(pdf_path, chunk_size)\n",
    "        self.create_embeddings()\n",
    "        \n",
    "    def extract_and_process_pdf(self, pdf_path: str, chunk_size: int):\n",
    "        \"\"\"Extract text and tables from PDF\"\"\"\n",
    "        try:\n",
    "            # Extract text using pdfplumber\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                for page_num, page in enumerate(pdf.pages, 1):\n",
    "                    # Extract text\n",
    "                    text = page.extract_text()\n",
    "                    if text:\n",
    "                        self.process_text(text, chunk_size, pdf_path, page_num)\n",
    "                    \n",
    "                    # Extract tables\n",
    "                    tables = page.extract_tables()\n",
    "                    if tables:\n",
    "                        self.process_tables(tables, pdf_path, page_num)\n",
    "            \n",
    "            # Extract tables using tabula\n",
    "            try:\n",
    "                tables = tabula.read_pdf(pdf_path, pages='all')\n",
    "                for table in tables:\n",
    "                    self.tables_data.append({\n",
    "                        'table': table,\n",
    "                        'source': pdf_path,\n",
    "                        'page': 'N/A'\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Table extraction with tabula failed: {str(e)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing PDF {pdf_path}: {str(e)}\")\n",
    "            \n",
    "    def process_text(self, text: str, chunk_size: int, source: str, page: int):\n",
    "        \"\"\"Process and chunk text\"\"\"\n",
    "        # Clean text\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        # Split into sentences\n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "        current_chunk = []\n",
    "        current_size = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence:\n",
    "                continue\n",
    "                \n",
    "            current_chunk.append(sentence)\n",
    "            current_size += len(sentence)\n",
    "            \n",
    "            if current_size >= chunk_size:\n",
    "                chunk_text = ' '.join(current_chunk)\n",
    "                self.chunks.append(chunk_text)\n",
    "                self.metadata.append({\n",
    "                    'source': source,\n",
    "                    'page': page,\n",
    "                    'type': 'text'\n",
    "                })\n",
    "                current_chunk = []\n",
    "                current_size = 0\n",
    "                \n",
    "        if current_chunk:\n",
    "            chunk_text = ' '.join(current_chunk)\n",
    "            self.chunks.append(chunk_text)\n",
    "            self.metadata.append({\n",
    "                'source': source,\n",
    "                'page': page,\n",
    "                'type': 'text'\n",
    "            })\n",
    "            \n",
    "    def process_tables(self, tables: List, source: str, page: int):\n",
    "        \"\"\"Process extracted tables\"\"\"\n",
    "        for table in tables:\n",
    "            if table and len(table) > 0:\n",
    "                try:\n",
    "                    # Convert table to DataFrame\n",
    "                    df = pd.DataFrame(table[1:], columns=table[0])\n",
    "                    # Clean DataFrame\n",
    "                    df = df.dropna(how='all').dropna(axis=1, how='all')\n",
    "                    \n",
    "                    if not df.empty:\n",
    "                        self.tables_data.append({\n",
    "                            'table': df,\n",
    "                            'source': source,\n",
    "                            'page': page\n",
    "                        })\n",
    "                        \n",
    "                        # Add table content to chunks for searching\n",
    "                        table_text = df.to_string()\n",
    "                        self.chunks.append(table_text)\n",
    "                        self.metadata.append({\n",
    "                            'source': source,\n",
    "                            'page': page,\n",
    "                            'type': 'table'\n",
    "                        })\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Failed to process table: {str(e)}\")\n",
    "                \n",
    "    def create_embeddings(self):\n",
    "        \"\"\"Create embeddings for chunks\"\"\"\n",
    "        if not self.chunks:\n",
    "            print(\"No chunks to embed!\")\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            self.embeddings = self.embed_model.encode(self.chunks)\n",
    "            dimension = self.embeddings.shape[1]\n",
    "            self.index = faiss.IndexFlatL2(dimension)\n",
    "            self.index.add(self.embeddings.astype('float32'))\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating embeddings: {str(e)}\")\n",
    "            \n",
    "    def search(self, query: str, k: int = 3) -> List[Dict]:\n",
    "        \"\"\"Search for relevant chunks\"\"\"\n",
    "        if not self.index:\n",
    "            return []\n",
    "            \n",
    "        try:\n",
    "            query_embedding = self.embed_model.encode([query])\n",
    "            distances, indices = self.index.search(query_embedding.astype('float32'), k)\n",
    "            \n",
    "            results = []\n",
    "            for i, idx in enumerate(indices[0]):\n",
    "                results.append({\n",
    "                    'chunk': self.chunks[idx],\n",
    "                    'metadata': self.metadata[idx],\n",
    "                    'score': float(distances[0][i])\n",
    "                })\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"Error during search: {str(e)}\")\n",
    "            return []\n",
    "            \n",
    "    def handle_comparison_query(self, query: str) -> str:\n",
    "        \"\"\"Handle comparison queries\"\"\"\n",
    "        relevant_chunks = self.search(query, k=5)\n",
    "        relevant_tables = self.find_relevant_tables(query)\n",
    "        \n",
    "        comparison_result = \"Comparison Analysis:\\n\\n\"\n",
    "        \n",
    "        # Process text-based comparisons\n",
    "        if relevant_chunks:\n",
    "            comparison_result += \"Text Analysis:\\n\"\n",
    "            for chunk in relevant_chunks:\n",
    "                comparison_result += f\"- From {chunk['metadata']['source']} (Page {chunk['metadata']['page']}):\\n\"\n",
    "                comparison_result += f\"  {chunk['chunk']}\\n\\n\"\n",
    "                \n",
    "        # Process table-based comparisons\n",
    "        if relevant_tables:\n",
    "            comparison_result += \"\\nTabular Analysis:\\n\"\n",
    "            for table_info in relevant_tables:\n",
    "                comparison_result += f\"\\nTable from {table_info['source']} (Page {table_info['page']}):\\n\"\n",
    "                comparison_result += str(table_info['table'])\n",
    "                comparison_result += \"\\n\"\n",
    "                \n",
    "        return comparison_result\n",
    "        \n",
    "    def find_relevant_tables(self, query: str) -> List[Dict]:\n",
    "        \"\"\"Find relevant tables for the query\"\"\"\n",
    "        relevant_tables = []\n",
    "        query_terms = set(query.lower().split())\n",
    "        \n",
    "        for table_info in self.tables_data:\n",
    "            table_str = str(table_info['table']).lower()\n",
    "            if any(term in table_str for term in query_terms):\n",
    "                relevant_tables.append(table_info)\n",
    "                \n",
    "        return relevant_tables\n",
    "        \n",
    "    def get_answer(self, query: str) -> str:\n",
    "        \"\"\"Generate answer for query\"\"\"\n",
    "        try:\n",
    "            # Check if it's a comparison query\n",
    "            if any(word in query.lower() for word in ['compare', 'comparison', 'difference', 'versus', 'vs']):\n",
    "                return self.handle_comparison_query(query)\n",
    "                \n",
    "            # Regular query processing\n",
    "            relevant_chunks = self.search(query)\n",
    "            if not relevant_chunks:\n",
    "                return \"I couldn't find relevant information to answer your question.\"\n",
    "                \n",
    "            response = \"Based on the retrieved information:\\n\\n\"\n",
    "            for chunk in relevant_chunks:\n",
    "                response += f\"From {chunk['metadata']['source']} (Page {chunk['metadata']['page']}):\\n\"\n",
    "                response += f\"{chunk['chunk']}\\n\\n\"\n",
    "                \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error generating answer: {str(e)}\"\n",
    "\n",
    "def validate_pdf_path(path: str) -> bool:\n",
    "    \"\"\"Validate if the path exists and is a PDF file\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Error: File does not exist: {path}\")\n",
    "        return False\n",
    "    if not path.lower().endswith('.pdf'):\n",
    "        print(f\"Error: File is not a PDF: {path}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def get_pdf_paths() -> List[str]:\n",
    "    \"\"\"Get PDF paths from user input\"\"\"\n",
    "    pdf_paths = []\n",
    "    while True:\n",
    "        print(\"\\nEnter PDF file path (or 'done' when finished):\")\n",
    "        path = input().strip()\n",
    "        \n",
    "        if path.lower() == 'done':\n",
    "            if not pdf_paths:\n",
    "                print(\"Please enter at least one PDF path.\")\n",
    "                continue\n",
    "            break\n",
    "            \n",
    "        if validate_pdf_path(path):\n",
    "            pdf_paths.append(path)\n",
    "            print(f\"Added: {path}\")\n",
    "            print(\"Enter another PDF path or type 'done' to proceed\")\n",
    "        else:\n",
    "            print(\"Please enter a valid PDF file path\")\n",
    "    \n",
    "    return pdf_paths\n",
    "\n",
    "def main():\n",
    "    print(\"Welcome to the Advanced RAG System!\")\n",
    "    print(\"This system will help you analyze and query multiple PDF documents.\")\n",
    "    \n",
    "    # Get PDF paths from user\n",
    "    print(\"\\nFirst, let's specify the PDF files you want to analyze.\")\n",
    "    pdf_paths = get_pdf_paths()\n",
    "    \n",
    "    # Initialize the system\n",
    "    print(\"\\nInitializing the RAG system...\")\n",
    "    rag_system = AdvancedRAGSystem()\n",
    "    \n",
    "    # Process PDFs\n",
    "    print(\"\\nProcessing PDF files...\")\n",
    "    rag_system.process_pdfs(pdf_paths)\n",
    "    print(\"PDF processing completed!\")\n",
    "    \n",
    "    # Interactive query loop\n",
    "    print(\"\\nYou can now ask questions about your PDFs!\")\n",
    "    print(\"Available commands:\")\n",
    "    print(\"- Type your question to query the documents\")\n",
    "    print(\"- Type 'list' to show processed PDF files\")\n",
    "    print(\"- Type 'add' to add more PDF files\")\n",
    "    print(\"- Type 'quit' to exit\")\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\nEnter your question or command: \")\n",
    "        query = input().strip()\n",
    "        \n",
    "        if query.lower() == 'quit':\n",
    "            print(\"Thank you for using the RAG system. Goodbye!\")\n",
    "            break\n",
    "            \n",
    "        elif query.lower() == 'list':\n",
    "            print(\"\\nProcessed PDF files:\")\n",
    "            for i, path in enumerate(pdf_paths, 1):\n",
    "                print(f\"{i}. {path}\")\n",
    "            continue\n",
    "            \n",
    "        elif query.lower() == 'add':\n",
    "            new_paths = get_pdf_paths()\n",
    "            rag_system.process_pdfs(new_paths)\n",
    "            pdf_paths.extend(new_paths)\n",
    "            print(\"New PDFs added and processed!\")\n",
    "            continue\n",
    "            \n",
    "        # Handle the query\n",
    "        try:\n",
    "            answer = rag_system.get_answer(query)\n",
    "            print(\"\\nAnswer:\", answer)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing query: {str(e)}\")\n",
    "            print(\"Please try a different question or check if the PDFs were processed correctly.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nProgram terminated by user. Goodbye!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a958ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PROTEIN",
   "language": "python",
   "name": "protein"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
