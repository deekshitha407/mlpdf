{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db37f394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ursti\\anaconda3\\envs\\PROTEIN\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ursti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ursti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\ursti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\ursti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Website RAG System!\n",
      "This system will help you extract and query information from websites.\n",
      "\n",
      "Enter a website URL to analyze (or 'quit' to exit):\n",
      "hitam.org\n",
      "\n",
      "Crawling https://hitam.org...\n",
      "Found 10 pages\n",
      "Extracting and processing content...\n",
      "Creating embeddings...\n",
      "\n",
      "You can now ask questions about the website!\n",
      "\n",
      "Enter your question (or 'new' for new website, 'quit' to exit):\n",
      "what is the website used for\n",
      "\n",
      "Answer: Query: what is the website used for\n",
      "\n",
      "Relevant information:\n",
      "\n",
      "From https://hitam.org/international-partnerships/:\n",
      "Title: INTERNATIONAL PARTNERSHIPS  Hyderabad Institute of Technology and Management\n",
      "\n",
      "List items:\n",
      "It builds the academic strengths of the students with an emphasis on hands-on learning and research collaborations.. It helps to approach an interdisciplinary education allowing students to explore diverse fields and broaden their perspectives.. It equips students with relevant skills to align with industry demands.. It offers workshops, seminars, and conferences, to facilitate professional development.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "From https://hitam.org/about-cdc/:\n",
      "Title: About CDC  Hyderabad Institute of Technology and Management\n",
      "\n",
      "Content:\n",
      "About CDC  Hyderabad Institute of Technology and Management Hyderabad Institute of Technology and Management UGC Autonomous, Affiliated to JNTUH Accredited by NAACA, NBA EAMCET Counseling Code HITM About CDC Home About CDC Home About CDC Home Careers and Placements About Careers and Placements HITAMs Career Design Centre CDC serves as an enabler for students careers and ambitions. It works closely with industries and other departments internally to assist students in aligning their learning towards the needs of the industry. CDC conducts structured training programs from first year onwards to help students develop their aptitude, reasoning and soft skills. Careers aspirations for the students are collected every year to help them with personalised suggestions, along with an Individual Development Plan. Periodic assessments are conducted to assess the students learning and make them understand where they stand against the expectations of the industry. CDC offers industry relevant technical training based on the students aspirations.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "From https://hitam.org/academics/:\n",
      "Title: Academics  Hyderabad Institute of Technology and Management\n",
      "\n",
      "Content:\n",
      "Academics  Hyderabad Institute of Technology and Management Hyderabad Institute of Technology and Management UGC Autonomous, Affiliated to JNTUH Accredited by NAACA, NBA EAMCET Counseling Code HITM Academics Home Academics Academics Curriculum Assessment And Padagogy CAP Our goal is to provide students with a curriculum that not only meets industry standards but also goes beyond just simple chalk and talk. Our faculty employs Problem-Based Learning PBL throughout the educational framework, assigning students eight projects aligned with their career aspirations over the course of their studies. This approach ensures that students graduate industry-ready, with sizeable projects to their name at the beginning of their professional careers. ACCREDITATIONS We are recognized for excellence with NAAC A grade and NBA. Explore more. AWARDS Our Doing Engineering approach earned us bragging rights over prestigious awards on a national and global scale. RANKINGS We are positioned among the top private and autonomous institutions within the country.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "From https://hitam.org/about-hitam/:\n",
      "Title: ABOUT HITAM  Hyderabad Institute of Technology and Management\n",
      "\n",
      "Content:\n",
      "Alongside our state-of-the-art facilities, including labs, libraries, and classrooms, we offer sports amenities, medical dispensary, hygienic canteen, and college buses, ensuring all basic student needs are met. Testimonials Hear it unfiltered from the people who have been a part of HITAMs journey. Strategic Plan Discover how our student-centric initiatives and dedicated support are our way forward in helping you Find Your Path Partnerships Explore how our strong ties with renowned foreign universities  industry-leading companies provide students and faculty valuable opportunities. VISION  To be a role model technological university of national repute that imparts research-based multi-disciplinary competencies in students to enable their career aspirations and contribute to society. MISSION Build students competencies through HITAMs Doing Engineering approach with relevant curriculum, pedagogy and assessment. Collaborate with industry and institutions for capacity building in research, innovation and real time knowledge.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "From https://hitam.org/about-hitam:\n",
      "Title: ABOUT HITAM  Hyderabad Institute of Technology and Management\n",
      "\n",
      "Content:\n",
      "Alongside our state-of-the-art facilities, including labs, libraries, and classrooms, we offer sports amenities, medical dispensary, hygienic canteen, and college buses, ensuring all basic student needs are met. Testimonials Hear it unfiltered from the people who have been a part of HITAMs journey. Strategic Plan Discover how our student-centric initiatives and dedicated support are our way forward in helping you Find Your Path Partnerships Explore how our strong ties with renowned foreign universities  industry-leading companies provide students and faculty valuable opportunities. VISION  To be a role model technological university of national repute that imparts research-based multi-disciplinary competencies in students to enable their career aspirations and contribute to society. MISSION Build students competencies through HITAMs Doing Engineering approach with relevant curriculum, pedagogy and assessment. Collaborate with industry and institutions for capacity building in research, innovation and real time knowledge.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Enter your question (or 'new' for new website, 'quit' to exit):\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from typing import List, Dict, Union\n",
    "import re\n",
    "import nltk\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import warnings\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "\n",
    "# Download all required NLTK data at startup\n",
    "def download_nltk_data():\n",
    "    try:\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('averaged_perceptron_tagger')\n",
    "        nltk.download('maxent_ne_chunker')\n",
    "        nltk.download('words')\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: NLTK download failed: {str(e)}\")\n",
    "\n",
    "# Download NLTK data\n",
    "download_nltk_data()\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class WebsiteRAGSystem:\n",
    "    def __init__(self):\n",
    "        self.embed_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "        self.chunks = []\n",
    "        self.embeddings = None\n",
    "        self.index = None\n",
    "        self.metadata = []\n",
    "        self.visited_urls = set()\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "\n",
    "    def simple_sentence_split(self, text: str) -> List[str]:\n",
    "        \"\"\"Fallback sentence splitter if NLTK fails\"\"\"\n",
    "        # Split on common sentence endings\n",
    "        splits = re.split(r'(?<=[.!?])\\s+', text)\n",
    "        return [s.strip() for s in splits if s.strip()]\n",
    "\n",
    "    def get_sentences(self, text: str) -> List[str]:\n",
    "        \"\"\"Get sentences with fallback method\"\"\"\n",
    "        try:\n",
    "            return nltk.sent_tokenize(text)\n",
    "        except Exception:\n",
    "            return self.simple_sentence_split(text)\n",
    "\n",
    "    def crawl_website(self, base_url: str, max_pages: int = 10) -> List[str]:\n",
    "        urls_to_visit = [base_url]\n",
    "        crawled_urls = []\n",
    "        \n",
    "        while urls_to_visit and len(crawled_urls) < max_pages:\n",
    "            url = urls_to_visit.pop(0)\n",
    "            if url in self.visited_urls:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                response = requests.get(url, headers=self.headers, timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                    self.visited_urls.add(url)\n",
    "                    crawled_urls.append(url)\n",
    "                    \n",
    "                    for link in soup.find_all('a', href=True):\n",
    "                        new_url = urljoin(base_url, link['href'])\n",
    "                        if (new_url.startswith(base_url) and \n",
    "                            new_url not in self.visited_urls and \n",
    "                            new_url not in urls_to_visit):\n",
    "                            urls_to_visit.append(new_url)\n",
    "                            \n",
    "                time.sleep(1)\n",
    "            except Exception as e:\n",
    "                print(f\"Error crawling {url}: {str(e)}\")\n",
    "                \n",
    "        return crawled_urls\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text\"\"\"\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Remove special characters\n",
    "        text = re.sub(r'[^\\w\\s.,!?-]', '', text)\n",
    "        return text.strip()\n",
    "\n",
    "    def extract_content(self, url: str) -> Dict:\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            if response.status_code != 200:\n",
    "                return None\n",
    "                \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Remove unwanted elements\n",
    "            for element in soup.find_all(['script', 'style', 'nav', 'footer']):\n",
    "                element.decompose()\n",
    "                \n",
    "            # Extract structured data\n",
    "            structured_data = {}\n",
    "            \n",
    "            # Find and extract tables\n",
    "            tables = []\n",
    "            for table in soup.find_all('table'):\n",
    "                try:\n",
    "                    df = pd.read_html(str(table))[0]\n",
    "                    tables.append(df.to_dict())\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Extract lists\n",
    "            lists = []\n",
    "            for list_elem in soup.find_all(['ul', 'ol']):\n",
    "                items = [self.clean_text(item.get_text()) for item in list_elem.find_all('li')]\n",
    "                if items:\n",
    "                    lists.append(items)\n",
    "            \n",
    "            # Extract headings\n",
    "            headings = []\n",
    "            for heading in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "                heading_text = self.clean_text(heading.get_text())\n",
    "                if heading_text:\n",
    "                    headings.append(heading_text)\n",
    "            \n",
    "            # Extract main content\n",
    "            text_content = self.clean_text(soup.get_text(separator=' '))\n",
    "            \n",
    "            # Extract metadata\n",
    "            title = soup.title.string if soup.title else ''\n",
    "            title = self.clean_text(title)\n",
    "            \n",
    "            meta_description = ''\n",
    "            meta_description_tag = soup.find('meta', attrs={'name': 'description'})\n",
    "            if meta_description_tag:\n",
    "                meta_description = self.clean_text(meta_description_tag.get('content', ''))\n",
    "                \n",
    "            return {\n",
    "                'url': url,\n",
    "                'title': title,\n",
    "                'meta_description': meta_description,\n",
    "                'text_content': text_content,\n",
    "                'tables': tables,\n",
    "                'lists': lists,\n",
    "                'headings': headings\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting content from {url}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def process_content(self, content: Dict, chunk_size: int = 1000):\n",
    "        if not content:\n",
    "            return\n",
    "            \n",
    "        # Process headings and metadata first\n",
    "        if content['headings']:\n",
    "            heading_text = ' '.join(content['headings'])\n",
    "            self.chunks.append(heading_text)\n",
    "            self.metadata.append({\n",
    "                'url': content['url'],\n",
    "                'title': content['title'],\n",
    "                'type': 'headings'\n",
    "            })\n",
    "            \n",
    "        # Process main text content\n",
    "        text = content['text_content']\n",
    "        sentences = self.get_sentences(text)\n",
    "        \n",
    "        current_chunk = []\n",
    "        current_size = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            current_chunk.append(sentence)\n",
    "            current_size += len(sentence)\n",
    "            \n",
    "            if current_size >= chunk_size:\n",
    "                chunk_text = ' '.join(current_chunk)\n",
    "                self.chunks.append(chunk_text)\n",
    "                self.metadata.append({\n",
    "                    'url': content['url'],\n",
    "                    'title': content['title'],\n",
    "                    'type': 'text'\n",
    "                })\n",
    "                current_chunk = []\n",
    "                current_size = 0\n",
    "                \n",
    "        if current_chunk:\n",
    "            chunk_text = ' '.join(current_chunk)\n",
    "            self.chunks.append(chunk_text)\n",
    "            self.metadata.append({\n",
    "                'url': content['url'],\n",
    "                'title': content['title'],\n",
    "                'type': 'text'\n",
    "            })\n",
    "            \n",
    "        # Process tables\n",
    "        for table in content['tables']:\n",
    "            table_str = str(table)\n",
    "            self.chunks.append(table_str)\n",
    "            self.metadata.append({\n",
    "                'url': content['url'],\n",
    "                'title': content['title'],\n",
    "                'type': 'table'\n",
    "            })\n",
    "            \n",
    "        # Process lists\n",
    "        for list_items in content['lists']:\n",
    "            list_str = '. '.join(list_items)\n",
    "            self.chunks.append(list_str)\n",
    "            self.metadata.append({\n",
    "                'url': content['url'],\n",
    "                'title': content['title'],\n",
    "                'type': 'list'\n",
    "            })\n",
    "\n",
    "    def create_embeddings(self):\n",
    "        if not self.chunks:\n",
    "            return\n",
    "            \n",
    "        self.embeddings = self.embed_model.encode(self.chunks)\n",
    "        dimension = self.embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatL2(dimension)\n",
    "        self.index.add(self.embeddings.astype('float32'))\n",
    "\n",
    "    def search(self, query: str, k: int = 5) -> List[Dict]:\n",
    "        query_embedding = self.embed_model.encode([query])\n",
    "        distances, indices = self.index.search(query_embedding.astype('float32'), k)\n",
    "        \n",
    "        results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            results.append({\n",
    "                'chunk': self.chunks[idx],\n",
    "                'metadata': self.metadata[idx],\n",
    "                'score': float(distances[0][i])\n",
    "            })\n",
    "        return results\n",
    "\n",
    "    def format_response(self, query: str, relevant_chunks: List[Dict]) -> str:\n",
    "        response = f\"Query: {query}\\n\\nRelevant information:\\n\\n\"\n",
    "        \n",
    "        # Group chunks by URL\n",
    "        url_chunks = {}\n",
    "        for chunk in relevant_chunks:\n",
    "            url = chunk['metadata']['url']\n",
    "            if url not in url_chunks:\n",
    "                url_chunks[url] = []\n",
    "            url_chunks[url].append(chunk)\n",
    "        \n",
    "        # Format response by URL\n",
    "        for url, chunks in url_chunks.items():\n",
    "            response += f\"From {url}:\\n\"\n",
    "            response += f\"Title: {chunks[0]['metadata']['title']}\\n\\n\"\n",
    "            \n",
    "            # Sort chunks by type\n",
    "            type_order = {'headings': 1, 'list': 2, 'text': 3, 'table': 4}\n",
    "            chunks.sort(key=lambda x: type_order.get(x['metadata']['type'], 999))\n",
    "            \n",
    "            for chunk in chunks:\n",
    "                chunk_type = chunk['metadata']['type']\n",
    "                content = chunk['chunk']\n",
    "                \n",
    "                if chunk_type == 'headings':\n",
    "                    response += f\"Headings:\\n{content}\\n\\n\"\n",
    "                elif chunk_type == 'list':\n",
    "                    response += f\"List items:\\n{content}\\n\\n\"\n",
    "                elif chunk_type == 'text':\n",
    "                    response += f\"Content:\\n{content}\\n\\n\"\n",
    "                elif chunk_type == 'table':\n",
    "                    response += f\"Table data:\\n{content}\\n\\n\"\n",
    "                    \n",
    "            response += \"-\" * 80 + \"\\n\"\n",
    "            \n",
    "        return response\n",
    "\n",
    "    def get_answer(self, query: str) -> str:\n",
    "        relevant_chunks = self.search(query)\n",
    "        if not relevant_chunks:\n",
    "            return \"I couldn't find relevant information to answer your question.\"\n",
    "            \n",
    "        return self.format_response(query, relevant_chunks)\n",
    "\n",
    "def main():\n",
    "    print(\"Welcome to the Website RAG System!\")\n",
    "    print(\"This system will help you extract and query information from websites.\")\n",
    "    \n",
    "    rag_system = WebsiteRAGSystem()\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\nEnter a website URL to analyze (or 'quit' to exit):\")\n",
    "        url = input().strip()\n",
    "        \n",
    "        if url.lower() == 'quit':\n",
    "            break\n",
    "            \n",
    "        if not url.startswith(('http://', 'https://')):\n",
    "            url = 'https://' + url\n",
    "            \n",
    "        try:\n",
    "            print(f\"\\nCrawling {url}...\")\n",
    "            crawled_urls = rag_system.crawl_website(url)\n",
    "            print(f\"Found {len(crawled_urls)} pages\")\n",
    "            \n",
    "            print(\"Extracting and processing content...\")\n",
    "            for url in crawled_urls:\n",
    "                content = rag_system.extract_content(url)\n",
    "                if content:\n",
    "                    rag_system.process_content(content)\n",
    "                    \n",
    "            print(\"Creating embeddings...\")\n",
    "            rag_system.create_embeddings()\n",
    "            \n",
    "            print(\"\\nYou can now ask questions about the website!\")\n",
    "            while True:\n",
    "                print(\"\\nEnter your question (or 'new' for new website, 'quit' to exit):\")\n",
    "                query = input().strip()\n",
    "                \n",
    "                if query.lower() == 'quit':\n",
    "                    return\n",
    "                if query.lower() == 'new':\n",
    "                    break\n",
    "                    \n",
    "                answer = rag_system.get_answer(query)\n",
    "                print(\"\\nAnswer:\", answer)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing website: {str(e)}\")\n",
    "            print(\"Please try a different website or check your internet connection.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nProgram terminated by user. Goodbye!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f38139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import numpy as np\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# import faiss\n",
    "# from typing import List, Dict, Union\n",
    "# import re\n",
    "# import nltk\n",
    "# from urllib.parse import urljoin, urlparse\n",
    "# import warnings\n",
    "# import time\n",
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "# import pandas as pd\n",
    "\n",
    "# # Download required NLTK data\n",
    "# try:\n",
    "#     nltk.data.find('tokenizers/punkt')\n",
    "# except LookupError:\n",
    "#     nltk.download('punkt')\n",
    "\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# class WebsiteRAGSystem:\n",
    "#     def __init__(self):\n",
    "#         self.embed_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "#         self.chunks = []\n",
    "#         self.embeddings = None\n",
    "#         self.index = None\n",
    "#         self.metadata = []\n",
    "#         self.visited_urls = set()\n",
    "#         self.headers = {\n",
    "#             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "#         }\n",
    "\n",
    "#     def crawl_website(self, base_url: str, max_pages: int = 10) -> List[str]:\n",
    "#         urls_to_visit = [base_url]\n",
    "#         crawled_urls = []\n",
    "        \n",
    "#         while urls_to_visit and len(crawled_urls) < max_pages:\n",
    "#             url = urls_to_visit.pop(0)\n",
    "#             if url in self.visited_urls:\n",
    "#                 continue\n",
    "                \n",
    "#             try:\n",
    "#                 response = requests.get(url, headers=self.headers, timeout=10)\n",
    "#                 if response.status_code == 200:\n",
    "#                     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#                     self.visited_urls.add(url)\n",
    "#                     crawled_urls.append(url)\n",
    "                    \n",
    "#                     for link in soup.find_all('a', href=True):\n",
    "#                         new_url = urljoin(base_url, link['href'])\n",
    "#                         if (new_url.startswith(base_url) and \n",
    "#                             new_url not in self.visited_urls and \n",
    "#                             new_url not in urls_to_visit):\n",
    "#                             urls_to_visit.append(new_url)\n",
    "                            \n",
    "#                 time.sleep(1)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error crawling {url}: {str(e)}\")\n",
    "                \n",
    "#         return crawled_urls\n",
    "\n",
    "#     def extract_content(self, url: str) -> Dict:\n",
    "#         try:\n",
    "#             response = requests.get(url, headers=self.headers, timeout=10)\n",
    "#             if response.status_code != 200:\n",
    "#                 return None\n",
    "                \n",
    "#             soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "#             # Remove unwanted elements\n",
    "#             for element in soup.find_all(['script', 'style', 'nav', 'footer']):\n",
    "#                 element.decompose()\n",
    "                \n",
    "#             # Extract structured data\n",
    "#             structured_data = {}\n",
    "            \n",
    "#             # Find and extract tables\n",
    "#             tables = []\n",
    "#             for table in soup.find_all('table'):\n",
    "#                 try:\n",
    "#                     df = pd.read_html(str(table))[0]\n",
    "#                     tables.append(df.to_dict())\n",
    "#                 except:\n",
    "#                     continue\n",
    "            \n",
    "#             # Extract lists\n",
    "#             lists = []\n",
    "#             for list_elem in soup.find_all(['ul', 'ol']):\n",
    "#                 items = [item.get_text(strip=True) for item in list_elem.find_all('li')]\n",
    "#                 lists.append(items)\n",
    "            \n",
    "#             # Extract headings\n",
    "#             headings = []\n",
    "#             for heading in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "#                 headings.append(heading.get_text(strip=True))\n",
    "            \n",
    "#             # Extract main content\n",
    "#             text_content = soup.get_text(separator=' ', strip=True)\n",
    "            \n",
    "#             # Extract metadata\n",
    "#             title = soup.title.string if soup.title else ''\n",
    "#             meta_description = ''\n",
    "#             meta_description_tag = soup.find('meta', attrs={'name': 'description'})\n",
    "#             if meta_description_tag:\n",
    "#                 meta_description = meta_description_tag.get('content', '')\n",
    "                \n",
    "#             return {\n",
    "#                 'url': url,\n",
    "#                 'title': title,\n",
    "#                 'meta_description': meta_description,\n",
    "#                 'text_content': text_content,\n",
    "#                 'tables': tables,\n",
    "#                 'lists': lists,\n",
    "#                 'headings': headings\n",
    "#             }\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error extracting content from {url}: {str(e)}\")\n",
    "#             return None\n",
    "\n",
    "#     def process_content(self, content: Dict, chunk_size: int = 1000):\n",
    "#         if not content:\n",
    "#             return\n",
    "            \n",
    "#         # Process headings and metadata first\n",
    "#         if content['headings']:\n",
    "#             heading_text = ' '.join(content['headings'])\n",
    "#             self.chunks.append(heading_text)\n",
    "#             self.metadata.append({\n",
    "#                 'url': content['url'],\n",
    "#                 'title': content['title'],\n",
    "#                 'type': 'headings'\n",
    "#             })\n",
    "            \n",
    "#         # Process main text content\n",
    "#         text = content['text_content']\n",
    "#         sentences = nltk.sent_tokenize(text)\n",
    "        \n",
    "#         current_chunk = []\n",
    "#         current_size = 0\n",
    "        \n",
    "#         for sentence in sentences:\n",
    "#             current_chunk.append(sentence)\n",
    "#             current_size += len(sentence)\n",
    "            \n",
    "#             if current_size >= chunk_size:\n",
    "#                 chunk_text = ' '.join(current_chunk)\n",
    "#                 self.chunks.append(chunk_text)\n",
    "#                 self.metadata.append({\n",
    "#                     'url': content['url'],\n",
    "#                     'title': content['title'],\n",
    "#                     'type': 'text'\n",
    "#                 })\n",
    "#                 current_chunk = []\n",
    "#                 current_size = 0\n",
    "                \n",
    "#         if current_chunk:\n",
    "#             chunk_text = ' '.join(current_chunk)\n",
    "#             self.chunks.append(chunk_text)\n",
    "#             self.metadata.append({\n",
    "#                 'url': content['url'],\n",
    "#                 'title': content['title'],\n",
    "#                 'type': 'text'\n",
    "#             })\n",
    "            \n",
    "#         # Process tables\n",
    "#         for table in content['tables']:\n",
    "#             table_str = str(table)\n",
    "#             self.chunks.append(table_str)\n",
    "#             self.metadata.append({\n",
    "#                 'url': content['url'],\n",
    "#                 'title': content['title'],\n",
    "#                 'type': 'table'\n",
    "#             })\n",
    "            \n",
    "#         # Process lists\n",
    "#         for list_items in content['lists']:\n",
    "#             list_str = '. '.join(list_items)\n",
    "#             self.chunks.append(list_str)\n",
    "#             self.metadata.append({\n",
    "#                 'url': content['url'],\n",
    "#                 'title': content['title'],\n",
    "#                 'type': 'list'\n",
    "#             })\n",
    "\n",
    "#     def create_embeddings(self):\n",
    "#         if not self.chunks:\n",
    "#             return\n",
    "            \n",
    "#         self.embeddings = self.embed_model.encode(self.chunks)\n",
    "#         dimension = self.embeddings.shape[1]\n",
    "#         self.index = faiss.IndexFlatL2(dimension)\n",
    "#         self.index.add(self.embeddings.astype('float32'))\n",
    "\n",
    "#     def search(self, query: str, k: int = 5) -> List[Dict]:\n",
    "#         query_embedding = self.embed_model.encode([query])\n",
    "#         distances, indices = self.index.search(query_embedding.astype('float32'), k)\n",
    "        \n",
    "#         results = []\n",
    "#         for i, idx in enumerate(indices[0]):\n",
    "#             results.append({\n",
    "#                 'chunk': self.chunks[idx],\n",
    "#                 'metadata': self.metadata[idx],\n",
    "#                 'score': float(distances[0][i])\n",
    "#             })\n",
    "#         return results\n",
    "\n",
    "#     def format_response(self, query: str, relevant_chunks: List[Dict]) -> str:\n",
    "#         response = f\"Query: {query}\\n\\nRelevant information:\\n\\n\"\n",
    "        \n",
    "#         # Group chunks by URL\n",
    "#         url_chunks = {}\n",
    "#         for chunk in relevant_chunks:\n",
    "#             url = chunk['metadata']['url']\n",
    "#             if url not in url_chunks:\n",
    "#                 url_chunks[url] = []\n",
    "#             url_chunks[url].append(chunk)\n",
    "        \n",
    "#         # Format response by URL\n",
    "#         for url, chunks in url_chunks.items():\n",
    "#             response += f\"From {url}:\\n\"\n",
    "#             response += f\"Title: {chunks[0]['metadata']['title']}\\n\\n\"\n",
    "            \n",
    "#             # Sort chunks by type (headings first, then lists, then text, then tables)\n",
    "#             type_order = {'headings': 1, 'list': 2, 'text': 3, 'table': 4}\n",
    "#             chunks.sort(key=lambda x: type_order.get(x['metadata']['type'], 999))\n",
    "            \n",
    "#             for chunk in chunks:\n",
    "#                 chunk_type = chunk['metadata']['type']\n",
    "#                 content = chunk['chunk']\n",
    "                \n",
    "#                 if chunk_type == 'headings':\n",
    "#                     response += f\"Headings:\\n{content}\\n\\n\"\n",
    "#                 elif chunk_type == 'list':\n",
    "#                     response += f\"List items:\\n{content}\\n\\n\"\n",
    "#                 elif chunk_type == 'text':\n",
    "#                     response += f\"Content:\\n{content}\\n\\n\"\n",
    "#                 elif chunk_type == 'table':\n",
    "#                     response += f\"Table data:\\n{content}\\n\\n\"\n",
    "                    \n",
    "#             response += \"-\" * 80 + \"\\n\"\n",
    "            \n",
    "#         return response\n",
    "\n",
    "#     def get_answer(self, query: str) -> str:\n",
    "#         relevant_chunks = self.search(query)\n",
    "#         if not relevant_chunks:\n",
    "#             return \"I couldn't find relevant information to answer your question.\"\n",
    "            \n",
    "#         return self.format_response(query, relevant_chunks)\n",
    "\n",
    "# def main():\n",
    "#     print(\"Welcome to the Website RAG System!\")\n",
    "#     print(\"This system will help you extract and query information from websites.\")\n",
    "    \n",
    "#     rag_system = WebsiteRAGSystem()\n",
    "    \n",
    "#     while True:\n",
    "#         print(\"\\nEnter a website URL to analyze (or 'quit' to exit):\")\n",
    "#         url = input().strip()\n",
    "        \n",
    "#         if url.lower() == 'quit':\n",
    "#             break\n",
    "            \n",
    "#         if not url.startswith(('http://', 'https://')):\n",
    "#             url = 'https://' + url\n",
    "            \n",
    "#         try:\n",
    "#             print(f\"\\nCrawling {url}...\")\n",
    "#             crawled_urls = rag_system.crawl_website(url)\n",
    "#             print(f\"Found {len(crawled_urls)} pages\")\n",
    "            \n",
    "#             print(\"Extracting and processing content...\")\n",
    "#             for url in crawled_urls:\n",
    "#                 content = rag_system.extract_content(url)\n",
    "#                 if content:\n",
    "#                     rag_system.process_content(content)\n",
    "                    \n",
    "#             print(\"Creating embeddings...\")\n",
    "#             rag_system.create_embeddings()\n",
    "            \n",
    "#             print(\"\\nYou can now ask questions about the website!\")\n",
    "#             while True:\n",
    "#                 print(\"\\nEnter your question (or 'new' for new website, 'quit' to exit):\")\n",
    "#                 query = input().strip()\n",
    "                \n",
    "#                 if query.lower() == 'quit':\n",
    "#                     return\n",
    "#                 if query.lower() == 'new':\n",
    "#                     break\n",
    "                    \n",
    "#                 answer = rag_system.get_answer(query)\n",
    "#                 print(\"\\nAnswer:\", answer)\n",
    "                \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing website: {str(e)}\")\n",
    "#             print(\"Please try a different website or check your internet connection.\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     try:\n",
    "#         main()\n",
    "#     except KeyboardInterrupt:\n",
    "#         print(\"\\nProgram terminated by user. Goodbye!\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\nAn unexpected error occurred: {str(e)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PROTEIN",
   "language": "python",
   "name": "protein"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
